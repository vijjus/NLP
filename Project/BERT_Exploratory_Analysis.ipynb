{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapted from:\n",
    "- https://blog.scaleway.com/2019/understanding-text-with-bert/\n",
    "- https://www.kaggle.com/christofhenkel/loading-bert-using-pytorch-with-tokenizer-apex\n",
    "- https://github.com/huggingface/transformers/blob/master/examples/run_squad.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertConfig, BertForQuestionAnswering, BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture of BERT Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of the BERT model is composed of an Embedding layer plus a BERT Encoder and a final pooler.\n",
    "\n",
    "The Bert Embedding layer has the following components and dimensions:\n",
    "\n",
    "- word embeddings: size of vocabulary by size of hidden state;\n",
    "\n",
    "- position embeddings: max. length of sentence (512 tokens) by size of hidden state;\n",
    "\n",
    "- token_type_embeddings: 2 by size of hidden state (this embedding is apparently used for token-level binary classification, such as probability that a given token is the beginning/end of a span);\n",
    "\n",
    "- LayerNorm: ???\n",
    "\n",
    "- Dropout: dropout on embedding layer???\n",
    "\n",
    "The Bert Encoder is composed of 12 Bert Layers stacked on top of each other. Each Bert Layer is a replica of the same structure:\n",
    "\n",
    "- Bert self attention: query (size of hidden state by size of hidden state); key (size of hidden state by size of hidden state); value (size of hidden state by size of hidden state); dropout;\n",
    "\n",
    "- Bert self attention output: dense affine layer (size of hidden state by size of hidden state); LayerNorm; dropout;\n",
    "\n",
    "- Bert layer intermediate: dense affine layer (size of hidden state by 4 * size of hidden state; the 4 is presumably due to the concatenation of query, key, value and self attention output hidden state vectors);\n",
    "\n",
    "- Bert layer output: dense affine layer (4 * size of hidden state by size of hidden state); dropout;\n",
    "\n",
    "The pooler is a dense affine layer (size of hidden state by size of hidden state) with an tanh activation function;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").cuda()\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture of BERT for Question Answering: BERT Encoder + top dense affine layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model_QA = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "bert_model_QA.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below loads the tokenizer pre-trained on a specific vocabulary (in this case `bert-base-uncased`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below tokenizes one sample sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', ',', 'my', 'name', 'is', 'luca', '!', 'i', 'do', 'not', 'live', 'on', 'the', 'second', 'floor', ',', 'but', 'on', 'the', '5th', 'floor', 'instead', '.']\n",
      "The number of tokens is: 23\n"
     ]
    }
   ],
   "source": [
    "text = 'Hi, my name is Luca! I do not live on the second floor, but on the 5th floor instead.'\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "print('The number of tokens is: {}'.format(len(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below converts the sample sentence to BERT-formatted word ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7632,\n",
       " 1010,\n",
       " 2026,\n",
       " 2171,\n",
       " 2003,\n",
       " 15604,\n",
       " 999,\n",
       " 1045,\n",
       " 2079,\n",
       " 2025,\n",
       " 2444,\n",
       " 2006,\n",
       " 1996,\n",
       " 2117,\n",
       " 2723,\n",
       " 1010,\n",
       " 2021,\n",
       " 2006,\n",
       " 1996,\n",
       " 4833,\n",
       " 2723,\n",
       " 2612,\n",
       " 1012]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_output = bert_model(torch.tensor([input_ids]).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'tuple'> of length: 2.\n"
     ]
    }
   ],
   "source": [
    "print('type: {} of length: {}.'.format(type(bert_output), len(bert_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.4687e-01, -3.5512e-01, -6.8854e-01,  7.8208e-01,  2.1600e-01,\n",
       "         -1.3680e-01,  6.5701e-01,  2.6315e-01,  4.1703e-02, -9.9915e-01,\n",
       "          3.2556e-01,  6.0464e-01,  9.7480e-01,  3.9086e-01,  9.3562e-01,\n",
       "         -7.0522e-01, -2.5194e-01, -5.1356e-01,  1.9722e-01, -1.4919e-01,\n",
       "          8.2281e-01,  9.9965e-01,  3.0079e-01,  3.1392e-01,  2.7452e-01,\n",
       "          5.9897e-01, -5.4306e-01,  9.2878e-01,  9.0739e-01,  7.9554e-01,\n",
       "         -6.9410e-01, -5.1478e-02, -9.9209e-01, -1.6266e-01, -9.5810e-01,\n",
       "         -9.8643e-01,  3.2855e-01, -4.5272e-01, -1.5372e-01,  1.3056e-01,\n",
       "         -9.1916e-01,  2.9005e-01,  9.9928e-01, -4.4838e-01,  6.1825e-01,\n",
       "         -2.0422e-01, -9.9996e-01,  1.2594e-01, -8.9928e-01, -1.8152e-01,\n",
       "          5.8565e-01, -3.0661e-01,  8.1745e-02,  3.5404e-01,  3.0300e-01,\n",
       "         -3.5766e-01, -1.3039e-01,  7.4340e-02, -1.0266e-01, -4.4181e-01,\n",
       "         -4.7261e-01,  4.7402e-01, -6.0517e-01, -8.1791e-01, -2.1128e-01,\n",
       "          5.2509e-01, -2.6170e-01, -3.4482e-01,  1.0975e-01, -7.0565e-02,\n",
       "          6.2141e-01,  2.1670e-01,  2.7213e-01, -8.9195e-01, -1.7028e-01,\n",
       "          2.7780e-01, -5.8202e-01,  1.0000e+00, -3.8146e-01, -9.7945e-01,\n",
       "          8.7124e-01,  7.0577e-02,  5.6167e-01,  3.2969e-01,  3.3439e-01,\n",
       "         -1.0000e+00,  3.9413e-02, -3.0690e-01, -9.8781e-01,  4.5910e-02,\n",
       "          4.9248e-01,  5.4379e-02,  8.8000e-01,  5.2119e-01, -6.3013e-01,\n",
       "         -3.9840e-01, -1.6935e-01, -6.3422e-01, -2.6569e-01, -1.0397e-01,\n",
       "          1.0062e-01, -1.6013e-01, -3.0676e-01, -2.1504e-01,  2.5485e-01,\n",
       "         -3.1169e-01, -5.1473e-01,  2.5890e-01,  8.0475e-02,  4.9002e-01,\n",
       "          3.9563e-01, -3.0720e-01,  2.3291e-01, -8.8616e-01,  2.8008e-01,\n",
       "         -3.3903e-01, -9.7269e-01, -5.7040e-01, -9.8969e-01,  6.5722e-01,\n",
       "         -3.5837e-01, -3.3025e-01,  9.2735e-01, -4.7933e-01,  2.6581e-01,\n",
       "         -1.4081e-01,  8.2767e-02, -1.0000e+00, -6.3003e-02, -6.9917e-01,\n",
       "          3.2809e-01, -1.8953e-01, -9.7150e-01, -9.6382e-01,  4.7436e-01,\n",
       "          8.8478e-01,  7.5965e-02,  9.9816e-01, -2.6654e-01,  9.3658e-01,\n",
       "          3.9496e-01, -3.6188e-01, -2.3443e-01, -3.3767e-01,  6.9123e-01,\n",
       "          1.1910e-01, -6.9193e-01,  1.1059e-01, -3.2001e-01, -4.3543e-01,\n",
       "         -7.3714e-01, -1.5273e-01, -5.8193e-01, -9.0711e-01, -2.1939e-01,\n",
       "          9.5957e-01, -3.7732e-01, -4.0317e-01,  4.5217e-01, -1.4564e-01,\n",
       "         -4.9353e-01,  7.0193e-01,  6.2993e-01,  3.1610e-01, -1.8215e-01,\n",
       "          9.1619e-02,  1.0457e-01,  3.4499e-01, -7.6739e-01,  5.8463e-01,\n",
       "          2.4786e-01, -1.4890e-01, -8.1376e-01, -9.7158e-01, -2.1759e-01,\n",
       "          3.5999e-01,  9.8344e-01,  5.1848e-01,  2.1076e-01, -2.2198e-01,\n",
       "         -2.1639e-01, -1.8766e-02, -9.6068e-01,  9.7418e-01, -1.8266e-01,\n",
       "          1.0841e-01, -6.5313e-01, -1.5138e-01, -8.1804e-01,  5.5293e-01,\n",
       "          5.9204e-01,  3.2853e-01, -6.5166e-01, -8.6568e-02, -5.6167e-01,\n",
       "         -1.5728e-01, -7.2796e-01,  3.8788e-01, -1.4991e-01, -2.7260e-01,\n",
       "         -3.2780e-01,  9.4632e-01,  8.2443e-01,  5.3686e-01,  4.3260e-02,\n",
       "          5.0472e-01, -8.7281e-01, -5.4080e-01, -4.5197e-03,  1.5389e-01,\n",
       "         -6.4389e-02,  9.7986e-01, -7.9084e-01,  4.0866e-02, -8.2012e-01,\n",
       "         -9.7670e-01, -7.0420e-02, -7.9325e-01, -2.5620e-01, -6.9268e-01,\n",
       "          5.7349e-01, -8.3658e-02, -4.8948e-01,  2.4553e-01, -7.8633e-01,\n",
       "         -7.7121e-01,  3.1095e-01, -2.7920e-01,  3.0942e-01, -1.5167e-01,\n",
       "          9.4607e-01,  8.6189e-01, -6.5235e-01,  5.2412e-01,  9.6943e-01,\n",
       "         -9.2713e-01, -7.3608e-01,  7.6041e-01, -2.0095e-01,  5.7554e-01,\n",
       "         -4.4919e-01,  9.7653e-01,  6.9540e-01,  3.7800e-01, -9.1961e-01,\n",
       "         -7.4978e-01, -5.8897e-01, -3.0803e-01, -5.5098e-02, -1.2659e-01,\n",
       "          1.6037e-01,  5.3755e-01,  8.9510e-02, -3.9529e-02, -2.1787e-01,\n",
       "          8.4346e-01, -9.8316e-01, -9.6488e-01, -7.4361e-01,  3.0137e-01,\n",
       "         -9.8590e-01,  7.8834e-01,  1.4689e-01,  6.6058e-01, -4.2430e-01,\n",
       "         -4.1365e-01, -9.6300e-01,  6.5442e-01,  1.9178e-01,  9.0698e-01,\n",
       "         -6.0226e-01, -7.9276e-01,  6.6563e-02, -9.5812e-01,  5.2239e-02,\n",
       "         -1.8301e-01,  2.1454e-01, -1.1535e-01, -9.0475e-01,  4.3214e-01,\n",
       "          5.9958e-01,  3.1370e-01, -4.6876e-01,  9.6326e-01,  9.9999e-01,\n",
       "          9.7375e-01,  8.4013e-01,  3.1046e-01, -9.9887e-01, -4.1704e-01,\n",
       "          9.9963e-01, -8.5341e-01, -1.0000e+00, -8.7975e-01, -5.8711e-01,\n",
       "          3.7288e-01, -1.0000e+00, -2.0082e-01,  5.1260e-02, -8.7558e-01,\n",
       "          1.1716e-01,  9.4833e-01,  9.1564e-01, -1.0000e+00,  6.8686e-01,\n",
       "          8.3526e-01, -4.9282e-01,  4.7104e-01, -4.4046e-01,  9.6249e-01,\n",
       "          5.5122e-01,  4.0383e-01, -1.7399e-01,  4.1405e-01, -8.8242e-01,\n",
       "         -7.8011e-01, -4.6309e-02, -6.4542e-01,  9.9168e-01,  1.7154e-01,\n",
       "         -5.6196e-01, -8.8777e-01,  3.7477e-01, -4.3759e-02, -3.4430e-02,\n",
       "         -9.6351e-01, -1.5512e-01, -4.1502e-01,  6.1009e-01,  1.9056e-01,\n",
       "          2.7445e-01, -5.7731e-01,  6.9680e-02,  3.6842e-01,  1.0207e-01,\n",
       "          5.4236e-01, -7.6936e-01, -2.8680e-01,  4.5265e-01, -6.7494e-01,\n",
       "         -4.2573e-01, -9.7017e-01,  9.4723e-01, -3.1621e-01, -3.1235e-01,\n",
       "          1.0000e+00,  2.0365e-01, -8.3091e-01,  4.9858e-01,  1.3122e-01,\n",
       "         -8.7326e-01,  1.0000e+00,  7.1303e-01, -9.8664e-01, -5.0213e-01,\n",
       "          3.3289e-01, -4.3313e-01, -5.0715e-01,  9.9651e-01, -2.6435e-02,\n",
       "         -1.1081e-01, -2.1684e-01,  9.9080e-01, -9.8524e-01,  9.7965e-01,\n",
       "         -7.3879e-01, -9.4572e-01,  9.2873e-01,  9.5534e-01, -6.3557e-01,\n",
       "         -4.7502e-01,  1.7399e-02, -5.7731e-01,  2.2103e-01, -7.0930e-01,\n",
       "          3.3930e-01,  6.4567e-01,  3.3925e-02,  9.1620e-01, -3.8494e-01,\n",
       "         -5.5721e-01, -1.9594e-02,  3.0466e-02,  6.1047e-01,  8.2189e-01,\n",
       "          3.3759e-01, -2.1143e-01, -2.4542e-02, -2.4892e-01, -7.7006e-01,\n",
       "         -9.0414e-01,  5.9115e-01,  1.0000e+00,  2.4714e-01,  7.6951e-01,\n",
       "          5.7910e-03,  1.4336e-02,  1.5872e-01,  2.0556e-01,  4.1418e-01,\n",
       "         -4.5799e-02, -7.2842e-01,  3.3418e-01, -8.2834e-01, -9.9158e-01,\n",
       "          6.1462e-01,  9.2201e-02, -1.4045e-01,  9.9950e-01,  5.2639e-02,\n",
       "          3.6592e-02,  4.6146e-02,  3.1315e-01,  1.6094e-01,  2.4378e-01,\n",
       "          2.5034e-01,  9.7741e-01, -5.1813e-02,  5.2846e-01,  5.6285e-01,\n",
       "         -3.0515e-01, -4.7556e-01, -5.6565e-01, -1.0612e-01, -9.5563e-01,\n",
       "         -2.6060e-02, -9.4512e-01,  9.7058e-01, -2.8157e-01,  1.5392e-01,\n",
       "          2.6520e-01,  6.2448e-01,  1.0000e+00, -7.5821e-01,  6.0154e-01,\n",
       "         -7.2972e-02,  7.2096e-01, -9.9719e-01, -7.3937e-01, -4.4357e-01,\n",
       "          9.4389e-02, -2.7064e-01, -2.5772e-01,  1.7847e-01, -9.4053e-01,\n",
       "         -1.3142e-01, -1.2371e-01, -9.4185e-01, -9.7855e-01,  8.6448e-02,\n",
       "          3.5334e-01, -8.1534e-04, -9.7370e-01, -5.4982e-01, -5.6676e-01,\n",
       "         -1.6257e-01, -4.1745e-01, -9.1760e-01,  5.5685e-01, -3.2136e-01,\n",
       "          1.3275e-01, -2.7354e-01,  5.3090e-01,  4.4115e-01,  4.5109e-01,\n",
       "         -4.9369e-01, -1.5232e-01, -2.2680e-01, -7.8916e-01,  7.1264e-01,\n",
       "         -7.1140e-01, -5.5725e-01, -1.6918e-01,  1.0000e+00, -5.0316e-01,\n",
       "          8.4221e-01,  6.3001e-01,  6.2854e-01, -1.5653e-01,  3.3102e-01,\n",
       "          9.1942e-01,  1.9918e-01, -5.2867e-01, -5.6945e-02, -5.6264e-02,\n",
       "         -1.1648e-01,  6.1134e-01,  5.4389e-01,  6.1804e-01,  8.4181e-01,\n",
       "          3.1809e-01,  1.5262e-01,  1.8901e-01,  7.8704e-02,  9.1775e-01,\n",
       "         -1.3109e-01, -1.6003e-01, -3.6164e-01, -1.4357e-01, -4.1336e-01,\n",
       "         -3.8584e-01,  1.0000e+00,  2.1210e-02,  4.3180e-01, -9.8393e-01,\n",
       "         -4.1844e-01, -6.7514e-01,  9.9996e-01,  8.6633e-01, -3.5407e-01,\n",
       "          4.7881e-01,  4.8269e-02, -9.5026e-02,  6.1272e-01, -1.1469e-01,\n",
       "         -1.4331e-01,  8.0192e-02,  2.2357e-02,  9.5679e-01, -4.1426e-01,\n",
       "         -9.8174e-01, -6.2600e-01,  1.6205e-01, -9.5324e-01,  9.9902e-01,\n",
       "         -4.4181e-01, -8.0500e-02, -1.7146e-01, -2.8006e-02,  1.0750e-01,\n",
       "          7.3316e-02, -9.6922e-01,  1.5693e-01,  3.1119e-02,  9.6868e-01,\n",
       "          1.8164e-01, -5.0799e-01, -8.8930e-01, -2.6814e-01,  5.0054e-01,\n",
       "         -2.4737e-01, -9.1980e-01,  9.7534e-01, -9.6209e-01,  1.0897e-01,\n",
       "          1.0000e+00,  1.8959e-01, -8.1247e-01,  1.3850e-01, -3.4414e-01,\n",
       "          1.9058e-01, -2.1870e-01,  4.8576e-01, -9.1955e-01, -3.5705e-01,\n",
       "         -2.8852e-01,  1.5577e-01, -1.6432e-01,  2.9712e-03,  3.7565e-01,\n",
       "          1.0629e-01, -4.2830e-01, -3.3697e-01, -6.1179e-02,  2.9642e-01,\n",
       "          6.0636e-01, -1.2548e-01, -1.6644e-01, -4.9969e-02, -2.2151e-03,\n",
       "         -9.3836e-01, -2.2504e-01, -3.1730e-01, -9.9981e-01,  5.0593e-01,\n",
       "         -1.0000e+00,  2.9561e-01, -1.4218e-01, -1.3981e-01,  8.3109e-01,\n",
       "          3.6890e-01,  2.9166e-01, -7.5665e-01, -1.2766e-01,  7.3207e-01,\n",
       "          6.5105e-01, -2.1902e-01,  7.1490e-01, -6.8087e-01,  2.8054e-01,\n",
       "         -2.5788e-01,  1.9419e-01, -1.9245e-01,  6.9320e-01, -1.7496e-01,\n",
       "          1.0000e+00, -9.3441e-03, -6.8994e-01, -7.9853e-01,  1.5099e-01,\n",
       "         -1.2610e-01,  9.9999e-01, -7.2948e-01, -9.3350e-01,  4.3275e-01,\n",
       "         -6.2646e-01, -8.5197e-01,  2.9579e-01,  4.7936e-04, -5.1976e-01,\n",
       "         -6.9574e-01,  9.0992e-01,  7.5764e-01, -5.0105e-01,  5.9536e-01,\n",
       "         -3.1829e-01, -5.5776e-01,  6.9077e-02,  2.1130e-01,  9.8484e-01,\n",
       "          3.1162e-01,  7.2074e-01, -2.3666e-01, -4.3907e-01,  9.6055e-01,\n",
       "          3.2198e-01,  3.0312e-01,  7.2494e-02,  1.0000e+00,  4.0047e-01,\n",
       "         -9.0207e-01, -6.7254e-02, -9.0031e-01, -2.0047e-01, -8.6570e-01,\n",
       "          1.2158e-01,  2.5881e-01,  8.8194e-01, -1.9765e-01,  9.3993e-01,\n",
       "          6.6122e-03, -1.0589e-01,  1.8561e-01, -5.1632e-02,  1.4908e-01,\n",
       "         -9.4590e-01, -9.8964e-01, -9.8676e-01,  3.0071e-01, -2.5314e-01,\n",
       "         -9.6295e-02,  2.5918e-01,  1.7058e-01,  1.8686e-01,  2.7765e-01,\n",
       "         -1.0000e+00,  9.4406e-01,  4.5913e-01,  5.7550e-01,  9.4340e-01,\n",
       "          5.0857e-01,  2.7187e-01,  1.3623e-01, -9.7859e-01, -8.2693e-01,\n",
       "         -3.1234e-01, -1.8077e-01,  7.3466e-01,  5.3117e-01,  8.8684e-01,\n",
       "          2.4028e-01, -2.8104e-01, -4.8558e-01, -6.1943e-01, -8.2906e-01,\n",
       "         -9.9052e-01,  2.6978e-01,  5.0108e-01, -7.0839e-01,  9.7665e-01,\n",
       "         -7.0907e-02, -2.9802e-01,  2.4648e-01, -3.6625e-01,  3.3643e-01,\n",
       "          6.1832e-01,  1.8954e-01, -2.4494e-02,  5.6600e-01,  8.8164e-01,\n",
       "          8.1786e-01,  9.8221e-01, -4.9066e-01,  6.1250e-01,  3.2870e-01,\n",
       "          5.1834e-01,  8.4853e-01, -8.7928e-01,  2.5333e-01,  5.4083e-01,\n",
       "         -3.5262e-01,  1.3079e-01, -1.2922e-01, -7.3087e-01,  5.7657e-01,\n",
       "         -3.1560e-01,  6.7753e-01, -3.2448e-01,  5.6484e-02, -3.7483e-01,\n",
       "         -4.2630e-02, -5.5808e-01, -3.4944e-01,  5.5298e-01,  5.8450e-01,\n",
       "          9.1123e-01,  6.9602e-01,  1.1191e-02, -4.1243e-01, -1.1101e-02,\n",
       "         -2.7532e-01, -9.2729e-01,  4.5499e-01, -1.5489e-01, -8.2991e-02,\n",
       "          7.0886e-01,  8.2705e-02,  8.7102e-01, -2.4042e-03, -4.6276e-02,\n",
       "         -2.1913e-01, -6.5485e-01,  6.8877e-01, -4.3454e-01, -5.4265e-01,\n",
       "         -6.3349e-01,  3.9300e-01,  2.5749e-01,  9.9969e-01, -5.9317e-01,\n",
       "          1.6773e-01, -1.7399e-01, -1.4088e-01,  4.2697e-01, -4.6287e-01,\n",
       "         -1.0000e+00,  3.5097e-01,  5.1449e-01,  5.5708e-01, -3.3462e-01,\n",
       "          6.1821e-01,  4.1402e-01, -9.2088e-01, -3.7365e-01,  5.1239e-01,\n",
       "          5.6152e-01, -2.8344e-01,  3.9974e-01,  5.1382e-01,  9.5284e-01,\n",
       "          6.7860e-01,  5.8854e-01, -1.8406e-01,  1.8478e-01,  5.9334e-01,\n",
       "         -5.7721e-01, -5.1382e-01,  8.8305e-01]], device='cuda:0',\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 12 elements in bert_output[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 0.0254, -0.1865, -0.0811,  ...,  0.0142, -0.5408, -0.1748],\n",
       "          [ 0.0422,  0.1128, -0.1998,  ...,  0.5271,  0.5992,  0.1966],\n",
       "          [ 0.2058,  0.5050, -0.3177,  ..., -1.3612,  0.3292, -0.2163],\n",
       "          ...,\n",
       "          [ 1.2883,  0.4178, -0.1631,  ...,  0.2341,  0.2116, -1.2128],\n",
       "          [-1.1631,  0.6263, -1.3262,  ...,  0.5443,  0.5697,  0.0098],\n",
       "          [ 0.1227,  0.3578, -0.2816,  ...,  0.3877,  0.4342,  0.2016]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.0496, -0.3631, -0.3137,  ...,  0.1796, -0.1739, -0.1072],\n",
       "          [-0.0724,  0.0648, -0.1184,  ...,  0.7879,  0.3994,  0.0904],\n",
       "          [ 0.3511,  0.4609,  0.1004,  ..., -1.0712,  0.4891, -0.5336],\n",
       "          ...,\n",
       "          [ 0.9920,  0.7816, -0.5033,  ...,  0.4102,  0.2895, -0.7581],\n",
       "          [-1.3341,  0.9566, -1.5386,  ..., -0.0635,  1.1035,  0.1095],\n",
       "          [ 0.1019,  0.6002,  0.1395,  ...,  0.2404,  0.3441,  0.2954]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.0519, -0.3623, -0.0056,  ...,  0.3642,  0.0025,  0.0797],\n",
       "          [ 0.0509,  0.2511,  0.3140,  ...,  0.5229,  0.4741,  0.1977],\n",
       "          [ 0.3250, -0.1737,  0.4203,  ..., -0.4155,  0.1737, -0.1102],\n",
       "          ...,\n",
       "          [ 1.1389,  1.0839, -0.4932,  ...,  0.2428,  0.5924, -0.7440],\n",
       "          [-0.4886,  1.0429, -1.4732,  ..., -0.2048,  1.3227,  0.0513],\n",
       "          [ 0.3881,  0.3635,  0.4183,  ...,  0.0922,  0.6514,  0.2171]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.1761, -0.4382, -0.3738,  ...,  0.5082,  0.0991,  0.4220],\n",
       "          [-0.1388, -0.0767,  0.4408,  ...,  0.6977,  0.2772,  0.7239],\n",
       "          [ 0.5755, -0.3533,  0.2046,  ..., -0.0682,  0.0416,  0.1329],\n",
       "          ...,\n",
       "          [ 0.9713,  0.7115, -0.5132,  ..., -0.3955,  0.0938, -1.0296],\n",
       "          [-0.2177,  0.6424, -1.3753,  ..., -0.3109,  1.1280, -0.2609],\n",
       "          [ 0.4566, -0.0015,  0.6350,  ...,  0.1937,  0.8371,  0.1454]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.0479, -0.3876, -0.2695,  ...,  0.1170,  0.3402,  0.6243],\n",
       "          [ 0.0752,  0.2441,  0.3468,  ..., -0.0447,  0.5526,  0.9051],\n",
       "          [ 0.6699, -0.0581,  0.1853,  ..., -0.2055, -0.1802,  0.6963],\n",
       "          ...,\n",
       "          [ 1.1130,  0.7829, -0.5585,  ..., -0.3638,  0.4137, -0.7564],\n",
       "          [-0.2316,  0.5394, -0.9010,  ..., -0.6265,  1.6201,  0.1847],\n",
       "          [ 0.8711, -0.0465,  0.8695,  ..., -0.0264,  0.8188,  0.0240]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.1727, -0.6599, -0.2325,  ..., -0.4478,  0.2854,  0.3284],\n",
       "          [-0.1029,  0.2955,  0.0244,  ..., -0.2885,  0.1291,  1.0430],\n",
       "          [ 0.3762,  0.0403, -0.1487,  ...,  0.1227, -0.3738,  0.8551],\n",
       "          ...,\n",
       "          [ 0.9365,  0.5175, -0.3187,  ..., -0.3791,  0.1243, -0.5764],\n",
       "          [-0.0065,  0.4933, -0.6965,  ..., -0.8618,  1.4664,  0.5022],\n",
       "          [ 0.9185, -0.2163,  0.4534,  ..., -0.6521,  0.3538,  0.2530]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.0146, -0.5218, -0.0902,  ..., -0.3884,  0.4110,  0.4071],\n",
       "          [ 0.1324,  0.1942, -0.0992,  ..., -0.1510,  0.6788,  0.9075],\n",
       "          [ 0.8391, -0.2635, -0.3740,  ..., -0.1472,  0.8244,  0.9504],\n",
       "          ...,\n",
       "          [ 0.8178,  0.3781, -0.4959,  ..., -0.4448,  1.0731, -1.3195],\n",
       "          [ 0.3313,  0.4111, -1.1096,  ..., -0.3314,  2.3083,  0.6068],\n",
       "          [ 0.4462, -0.2312, -0.1584,  ..., -0.5036,  1.4397,  0.3978]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.0229, -0.1430,  0.2089,  ..., -0.0968,  0.4045,  0.4324],\n",
       "          [ 0.0351,  0.5306,  0.3678,  ..., -0.2573,  0.5207,  0.7156],\n",
       "          [ 0.4072,  0.0495, -0.1346,  ...,  0.1372,  0.6040,  1.4154],\n",
       "          ...,\n",
       "          [ 0.5816,  0.6080,  0.0203,  ..., -0.0822,  0.7249, -0.7870],\n",
       "          [-0.1990,  0.1073, -0.3739,  ..., -0.4289,  2.1189,  0.6647],\n",
       "          [-0.1843, -0.2880, -0.1781,  ...,  0.0669,  1.3633,  0.2904]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.0378,  0.0486,  0.3172,  ..., -0.3005,  0.1700,  0.1839],\n",
       "          [-0.4065,  0.6086,  0.6871,  ..., -0.9025,  0.5897,  0.2721],\n",
       "          [-0.1062,  0.3199,  0.3011,  ..., -0.2994,  0.5911,  1.1802],\n",
       "          ...,\n",
       "          [ 0.3555,  0.6403,  0.2323,  ..., -0.2417,  0.9172, -1.2390],\n",
       "          [-0.2499,  0.0774, -0.0712,  ..., -0.7602,  1.7095,  0.5893],\n",
       "          [-0.2780, -0.3894,  0.2856,  ..., -0.2593,  1.0168,  0.0832]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.0596, -0.3669,  0.6473,  ...,  0.0838,  0.3513,  0.4263],\n",
       "          [-0.3957,  0.3805,  0.7227,  ..., -0.9425,  0.8753,  0.4391],\n",
       "          [ 0.2410,  0.0191,  0.3256,  ..., -0.3291,  0.5189,  1.3633],\n",
       "          ...,\n",
       "          [-0.1298,  0.1699,  0.4866,  ..., -0.2390,  0.9129, -1.0412],\n",
       "          [-0.4271, -0.0799,  0.1572,  ..., -0.4316,  1.4002,  0.6174],\n",
       "          [-0.4360, -0.3878,  0.7710,  ..., -0.0135,  0.9476, -0.0762]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[ 2.8849e-01,  1.6417e-01,  4.2373e-01,  ..., -3.4663e-01,\n",
       "            3.2738e-01,  9.3156e-01],\n",
       "          [-2.5596e-01,  8.8651e-01,  5.0857e-01,  ..., -6.2071e-01,\n",
       "            8.0791e-01,  9.6603e-01],\n",
       "          [ 2.7275e-01,  7.3273e-01,  2.0275e-01,  ..., -3.5858e-01,\n",
       "            1.9872e-01,  2.0754e+00],\n",
       "          ...,\n",
       "          [ 3.9004e-03,  7.5256e-01, -2.4609e-04,  ..., -6.3462e-01,\n",
       "            9.8889e-01, -7.8637e-03],\n",
       "          [ 1.2020e-01,  3.0153e-01, -4.1858e-01,  ..., -8.8046e-01,\n",
       "            1.4079e+00,  1.4636e+00],\n",
       "          [-2.7472e-01, -2.2722e-01,  3.9287e-01,  ...,  1.6761e-01,\n",
       "            9.2626e-01, -3.6594e-02]]], device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.0500,  0.5176,  0.3170,  ..., -0.5147,  0.3445,  0.8903],\n",
       "          [-0.2042,  0.7380,  0.4011,  ..., -0.4998,  0.5254,  0.8762],\n",
       "          [-0.1559,  0.8274,  0.1780,  ..., -0.2038,  0.0221,  1.4586],\n",
       "          ...,\n",
       "          [-0.1945,  0.6219,  0.2148,  ..., -0.5717,  0.6080,  0.1845],\n",
       "          [-0.1612,  0.4702,  0.1959,  ..., -0.5780,  0.9934,  0.9168],\n",
       "          [-0.0887,  0.1486,  0.5231,  ..., -0.0932,  0.8672,  0.1894]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('there are {} elements in bert_output[0]'.format(len(bert_output[0])))\n",
    "bert_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of bert_output[0][0] is torch.Size([1, 23, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0254, -0.1865, -0.0811,  ...,  0.0142, -0.5408, -0.1748],\n",
       "         [ 0.0422,  0.1128, -0.1998,  ...,  0.5271,  0.5992,  0.1966],\n",
       "         [ 0.2058,  0.5050, -0.3177,  ..., -1.3612,  0.3292, -0.2163],\n",
       "         ...,\n",
       "         [ 1.2883,  0.4178, -0.1631,  ...,  0.2341,  0.2116, -1.2128],\n",
       "         [-1.1631,  0.6263, -1.3262,  ...,  0.5443,  0.5697,  0.0098],\n",
       "         [ 0.1227,  0.3578, -0.2816,  ...,  0.3877,  0.4342,  0.2016]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('the shape of bert_output[0][0] is {}'.format(bert_output[0][0].shape))\n",
    "bert_output[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0500,  0.5176,  0.3170,  ..., -0.5147,  0.3445,  0.8903],\n",
       "         [-0.2042,  0.7380,  0.4011,  ..., -0.4998,  0.5254,  0.8762],\n",
       "         [-0.1559,  0.8274,  0.1780,  ..., -0.2038,  0.0221,  1.4586],\n",
       "         ...,\n",
       "         [-0.1945,  0.6219,  0.2148,  ..., -0.5717,  0.6080,  0.1845],\n",
       "         [-0.1612,  0.4702,  0.1959,  ..., -0.5780,  0.9934,  0.9168],\n",
       "         [-0.0887,  0.1486,  0.5231,  ..., -0.0932,  0.8672,  0.1894]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_output[0][11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
