{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import argparse\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "import unicodedata\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.nn import embedding_lookup\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.ops.rnn_cell import DropoutWrapper\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "\n",
    "from text_helper import maybe_download\n",
    "from batch_helper import get_batch_generator\n",
    "\n",
    "importlib.reload(sys)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DATA_DIR=\"/users/vijay/MIDS/w266/Project/AnsweringMachines/dataset/\"\n",
    "SQUAD_BASE_URL = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PAD = b\"<pad>\"\n",
    "_UNK = b\"<unk>\"\n",
    "_START_VOCAB = [_PAD, _UNK]\n",
    "PAD_ID = 0\n",
    "UNK_ID = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_process(data_dir = DEFAULT_DATA_DIR):\n",
    "    \"\"\"\n",
    "    Produce 4 files per dataset (train & dev).\n",
    "    span: has the start & end span numbers for the answer (e.g. 34 35)\n",
    "    answer: the text version of span above (e.g. \"hundred yue\")\n",
    "    question: text tokens of the question (e.g. \"who populated the ...\")\n",
    "    context\": text tokens of the context paragraph (e.g. \"the area of modern zhejiang was ...\")\n",
    "    \"\"\"\n",
    "\n",
    "    print( \"Will download SQuAD datasets to {}\".format(data_dir))\n",
    "    print( \"Will put preprocessed SQuAD datasets in {}\".format(data_dir))\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    train_filename = \"train-v1.1.json\"\n",
    "    dev_filename = \"dev-v1.1.json\"\n",
    "\n",
    "    # download train set\n",
    "    maybe_download(SQUAD_BASE_URL, train_filename, data_dir, None)\n",
    "\n",
    "    # read train set\n",
    "    train_data = data_from_json(os.path.join(data_dir, train_filename))\n",
    "    print( \"Train data has %i examples total\" % total_exs(train_data))\n",
    "\n",
    "    # preprocess train set and write to file\n",
    "    preprocess_and_write(train_data, 'train', data_dir)\n",
    "\n",
    "    # download dev set\n",
    "    maybe_download(SQUAD_BASE_URL, dev_filename, data_dir, None)\n",
    "\n",
    "    # read dev set\n",
    "    dev_data = data_from_json(os.path.join(data_dir, dev_filename))\n",
    "    print( \"Dev data has %i examples total\" % total_exs(dev_data))\n",
    "\n",
    "    # preprocess dev set and write to file\n",
    "    preprocess_and_write(dev_data, 'dev', data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove(glove_path, embedding_size):\n",
    "    \"\"\"Reads from original GloVe .txt file and returns embedding matrix and\n",
    "    mappings from words to word ids.\n",
    "\n",
    "    Input:\n",
    "      glove_path: path to glove.6B.{glove_dim}d.txt\n",
    "      embedding_size: integer; needs to match the dimension in glove_path\n",
    "\n",
    "    Returns:\n",
    "      emb_matrix: Numpy array shape (400002, glove_dim) containing glove embeddings\n",
    "        (plus PAD and UNK embeddings in first two rows).\n",
    "        The rows of emb_matrix correspond to the word ids given in word2id and id2word\n",
    "      word2id: dictionary mapping word (string) to word id (int)\n",
    "      id2word: dictionary mapping word id (int) to word (string)\n",
    "    \"\"\"\n",
    "    print(\"Loading GloVe vectors from file: {:s}\".format(glove_path))\n",
    "    vocab_size = int(4e5 + 2)\n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_size + len(_START_VOCAB), embedding_size))\n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    \n",
    "    # initlialize the extra tokens\n",
    "    emb_matrix[:len(_START_VOCAB), :] = np.random.randn(len(_START_VOCAB), embedding_size)\n",
    "    \n",
    "    # add the extra tokens to the dictionaries\n",
    "    idx = 0\n",
    "    for word in _START_VOCAB:\n",
    "        word2id[word] = idx\n",
    "        id2word[idx] = word\n",
    "        idx += 1\n",
    "        \n",
    "    with open(glove_path, 'r') as fp:\n",
    "        for line in tqdm(fp, total=vocab_size):\n",
    "            line = line.lstrip().rstrip().split(\" \")\n",
    "            word = line[0]\n",
    "            vector = list(map(float, line[1:]))\n",
    "            assert(len(vector) == embedding_size)\n",
    "            emb_matrix[idx, :] = vector\n",
    "            word2id[word] = idx\n",
    "            id2word[idx] = word\n",
    "            idx += 1\n",
    "            \n",
    "    return emb_matrix, word2id, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear all flags so that the cell below can be be executed multiple times\n",
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()\n",
    "    keys_list = [keys for keys in flags_dict]\n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout=0.3\n",
    "batch_size=32\n",
    "hidden_size_encoder=150\n",
    "hidden_size_fully_connected=200\n",
    "context_len=300\n",
    "question_len=30\n",
    "embedding_size=100\n",
    "data_dir=DEFAULT_DATA_DIR\n",
    "num_epochs=10\n",
    "learning_rate=5e-5\n",
    "max_gradient_norm=5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3506/400002 [00:00<00:11, 35059.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe vectors from file: /users/vijay/MIDS/w266/Project/AnsweringMachines/dataset/glove.6B.100d.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 400000/400002 [00:09<00:00, 42277.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define path for glove vecs\n",
    "glove_path = os.path.join(data_dir, \"glove.6B.{}d.txt\".format(100))\n",
    "assert(os.path.exists(glove_path))\n",
    "\n",
    "\n",
    "# Load embedding matrix and vocab mappings\n",
    "emb_matrix, word2id, id2word = get_glove(glove_path, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNEncoder(object):\n",
    "    \"\"\"\n",
    "    General-purpose module to encode a sequence using a RNN.\n",
    "    It feeds the input through a RNN and returns all the hidden states.\n",
    "\n",
    "    Note: In lecture 8, we talked about how you might use a RNN as an \"encoder\"\n",
    "    to get a single, fixed size vector representation of a sequence\n",
    "    (e.g. by taking element-wise max of hidden states).\n",
    "    Here, we're using the RNN as an \"encoder\" but we're not taking max;\n",
    "    we're just returning all the hidden states. The terminology \"encoder\"\n",
    "    still applies because we're getting a different \"encoding\" of each\n",
    "    position in the sequence, and we'll use the encodings downstream in the model.\n",
    "\n",
    "    This code uses a bidirectional GRU, but you could experiment with other types of RNN.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, keep_prob):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          hidden_size: int. Hidden size of the RNN\n",
    "          keep_prob: Tensor containing a single scalar that is the keep probability (for dropout)\n",
    "        \"\"\"\n",
    "        self.hidden_size = hidden_size\n",
    "        self.keep_prob = keep_prob\n",
    "        self.rnn_cell_fw = tf.compat.v1.keras.layers.GRUCell(units=self.hidden_size)\n",
    "        #self.rnn_cell_fw = rnn_cell.GRUCell(num_units=self.hidden_size)\n",
    "        #self.rnn_cell_fw = DropoutWrapper(self.rnn_cell_fw, input_keep_prob=self.keep_prob)\n",
    "        self.rnn_cell_bw = tf.compat.v1.keras.layers.GRUCell(units=self.hidden_size)\n",
    "        #self.rnn_cell_bw = rnn_cell.GRUCell(num_units=self.hidden_size)\n",
    "        #self.rnn_cell_bw = DropoutWrapper(self.rnn_cell_bw, input_keep_prob=self.keep_prob)\n",
    "\n",
    "    def build_graph(self, inputs, masks, scopename):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          inputs: Tensor shape (batch_size, seq_len, input_size)\n",
    "          masks: Tensor shape (batch_size, seq_len).\n",
    "            Has 1s where there is real input, 0s where there's padding.\n",
    "            This is used to make sure tf.nn.bidirectional_dynamic_rnn doesn't iterate through masked steps.\n",
    "\n",
    "        Returns:\n",
    "          out: Tensor shape (batch_size, seq_len, hidden_size*2).\n",
    "            This is all hidden states (fw and bw hidden states are concatenated).\n",
    "        \"\"\"\n",
    "        with vs.variable_scope(scopename, reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "            input_lens = tf.reduce_sum(masks, reduction_indices=1) # shape (batch_size)\n",
    "\n",
    "            # Note: fw_out and bw_out are the hidden states for every timestep.\n",
    "            # Each is shape (batch_size, seq_len, hidden_size).\n",
    "            (fw_out, bw_out), _ = tf.nn.bidirectional_dynamic_rnn(self.rnn_cell_fw, \n",
    "                                                                  self.rnn_cell_bw,\n",
    "                                                                  inputs,\n",
    "                                                                  input_lens,\n",
    "                                                                  dtype=tf.float32) \n",
    "\n",
    "            # Concatenate the forward and backward hidden states\n",
    "            # The shape is now (batch_size, seq_len, hidden_size*2)\n",
    "            out = tf.concat([fw_out, bw_out], 2)\n",
    "\n",
    "            # Apply dropout\n",
    "            out = tf.nn.dropout(out, rate=1-self.keep_prob)\n",
    "\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(logits, mask, dim):\n",
    "    \"\"\"\n",
    "    Takes masked softmax over given dimension of logits.\n",
    "\n",
    "    Inputs:\n",
    "      logits: Numpy array. We want to take softmax over dimension dim.\n",
    "      mask: Numpy array of same shape as logits.\n",
    "        Has 1s where there's real data in logits, 0 where there's padding\n",
    "      dim: int. dimension over which to take softmax\n",
    "\n",
    "    Returns:\n",
    "      masked_logits: Numpy array same shape as logits.\n",
    "        This is the same as logits, but with 1e30 subtracted\n",
    "        (i.e. very large negative number) in the padding locations.\n",
    "      prob_dist: Numpy array same shape as logits.\n",
    "        The result of taking softmax over masked_logits in given dimension.\n",
    "        Should be 0 in padding locations.\n",
    "        Should sum to 1 over given dimension.\n",
    "    \"\"\"\n",
    "    exp_mask = (1 - tf.cast(mask, 'float')) * (-1e30) # -large where there's padding, 0 elsewhere\n",
    "    masked_logits = tf.add(logits, exp_mask) # where there's padding, set logits to -large\n",
    "    prob_dist = tf.nn.softmax(masked_logits, dim)\n",
    "    return masked_logits, prob_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSoftmaxLayer(object):\n",
    "    \"\"\"\n",
    "    Module to take set of hidden states, (e.g. one for each context location),\n",
    "    and return probability distribution over those states.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def build_graph(self, inputs, masks):\n",
    "        \"\"\"\n",
    "        Applies one linear downprojection layer, then softmax.\n",
    "\n",
    "        Inputs:\n",
    "          inputs: Tensor shape (batch_size, seq_len, hidden_size)\n",
    "          masks: Tensor shape (batch_size, seq_len)\n",
    "            Has 1s where there is real input, 0s where there's padding.\n",
    "\n",
    "        Outputs:\n",
    "          logits: Tensor shape (batch_size, seq_len)\n",
    "            logits is the result of the downprojection layer, but it has -1e30\n",
    "            (i.e. very large negative number) in the padded locations\n",
    "          prob_dist: Tensor shape (batch_size, seq_len)\n",
    "            The result of taking softmax over logits.\n",
    "            This should have 0 in the padded locations, and the rest should sum to 1.\n",
    "        \"\"\"\n",
    "        with vs.variable_scope(\"SimpleSoftmaxLayer\", reuse=tf.AUTO_REUSE):\n",
    "\n",
    "            # Linear downprojection layer\n",
    "            logits = tf.contrib.layers.fully_connected(inputs, num_outputs=1, activation_fn=None) # shape (batch_size, seq_len, 1)\n",
    "            logits = tf.squeeze(logits, axis=[2]) # shape (batch_size, seq_len)\n",
    "\n",
    "            # Take softmax over sequence\n",
    "            masked_logits, prob_dist = masked_softmax(logits, masks, 1)\n",
    "\n",
    "            return masked_logits, prob_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicAttn(object):\n",
    "    \"\"\"Module for basic attention.\n",
    "\n",
    "    Note: in this module we use the terminology of \"keys\" and \"values\" (see lectures).\n",
    "    In the terminology of \"X attends to Y\", \"keys attend to values\".\n",
    "\n",
    "    In the baseline model, the keys are the context hidden states\n",
    "    and the values are the question hidden states.\n",
    "\n",
    "    We choose to use general terminology of keys and values in this module\n",
    "    (rather than context and question) to avoid confusion if you reuse this\n",
    "    module with other inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, keep_prob, key_vec_size, value_vec_size):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          keep_prob: tensor containing a single scalar that is the keep probability (for dropout)\n",
    "          key_vec_size: size of the key vectors. int\n",
    "          value_vec_size: size of the value vectors. int\n",
    "        \"\"\"\n",
    "        self.keep_prob = keep_prob\n",
    "        self.key_vec_size = key_vec_size\n",
    "        self.value_vec_size = value_vec_size\n",
    "\n",
    "    def build_graph(self, values, values_mask, keys):\n",
    "        \"\"\"\n",
    "        Keys attend to values.\n",
    "        For each key, return an attention distribution and an attention output vector.\n",
    "\n",
    "        Inputs:\n",
    "          values: Tensor shape (batch_size, num_values, value_vec_size).\n",
    "          values_mask: Tensor shape (batch_size, num_values).\n",
    "            1s where there's real input, 0s where there's padding\n",
    "          keys: Tensor shape (batch_size, num_keys, value_vec_size)\n",
    "\n",
    "        Outputs:\n",
    "          attn_dist: Tensor shape (batch_size, num_keys, num_values).\n",
    "            For each key, the distribution should sum to 1,\n",
    "            and should be 0 in the value locations that correspond to padding.\n",
    "          output: Tensor shape (batch_size, num_keys, hidden_size).\n",
    "            This is the attention output; the weighted sum of the values\n",
    "            (using the attention distribution as weights).\n",
    "        \"\"\"\n",
    "        with vs.variable_scope(\"BasicAttn\", reuse=tf.AUTO_REUSE):\n",
    "\n",
    "            # Calculate attention distribution\n",
    "            values_t = tf.transpose(values, perm=[0, 2, 1]) # (batch_size, value_vec_size, num_values)\n",
    "            attn_logits = tf.matmul(keys, values_t) # shape (batch_size, num_keys, num_values)\n",
    "            print(\"Basic attn keys\", keys.shape)\n",
    "            print(\"Basic attn values\", values_t.shape)\n",
    "            print(\"Basic attn logits\", attn_logits.shape)\n",
    "            attn_logits_mask = tf.expand_dims(values_mask, 1) # shape (batch_size, 1, num_values)\n",
    "            _, attn_dist = masked_softmax(attn_logits, attn_logits_mask, 2) # shape (batch_size, num_keys, num_values). take softmax over values\n",
    "\n",
    "            # Use attention distribution to take weighted sum of values\n",
    "            output = tf.matmul(attn_dist, values) # shape (batch_size, num_keys, value_vec_size)\n",
    "\n",
    "            # Apply dropout\n",
    "            output = tf.nn.dropout(output, rate=1-self.keep_prob)\n",
    "\n",
    "            return attn_dist, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQuAD_Model(object):\n",
    "    \"\"\"Top-level Question Answering module\"\"\"\n",
    "\n",
    "    def __init__(self, id2word, word2id, emb_matrix):\n",
    "        \"\"\"\n",
    "        Initializes the QA model.\n",
    "\n",
    "        Inputs:\n",
    "          id2word: dictionary mapping word idx (int) to word (string)\n",
    "          word2id: dictionary mapping word (string) to word idx (int)\n",
    "          emb_matrix: numpy array shape (400002, embedding_size) containing pre-traing GloVe embeddings\n",
    "        \"\"\"\n",
    "        print(\"Initializing the SQuAD_Model...\")\n",
    "        \n",
    "        self.id2word = id2word\n",
    "        self.word2id = word2id\n",
    "        self.emb_matrix = emb_matrix\n",
    "        \n",
    "        with tf.variable_scope(\"QAModel\", \n",
    "                               initializer=tf.contrib.layers.variance_scaling_initializer(factor=1.0, uniform=True),\n",
    "                               reuse=tf.AUTO_REUSE):\n",
    "            self.context_ids = tf.placeholder(tf.int32, shape=[None, context_len])\n",
    "            self.context_mask = tf.placeholder(tf.int32, shape=[None, context_len])\n",
    "            self.question_ids = tf.placeholder(tf.int32, shape=[None, question_len])\n",
    "            self.question_mask = tf.placeholder(tf.int32, shape=[None, question_len])\n",
    "            self.answer_span = tf.placeholder(tf.int32, shape=[None, 2])\n",
    "        \n",
    "            # Add a placeholder to feed in the keep probability (for dropout).\n",
    "            # This is necessary so that we can instruct the model to use dropout when training, but not when testing\n",
    "            self.keep_prob = tf.placeholder_with_default(1.0, shape=())\n",
    "            self.hidden_size_fully_connected = hidden_size_fully_connected\n",
    "        \n",
    "            # here we setup functions used to output vectors using the embedding matrix\n",
    "            # self.context_embeds & self.question_embeds\n",
    "            self.add_embedding_layer(emb_matrix)\n",
    "            \n",
    "            # build the computational graph (similar to compiling the model?)\n",
    "            self.build_graph()\n",
    "            \n",
    "            \n",
    "            self.add_loss()\n",
    "        \n",
    "        # Define trainable parameters, gradient, gradient norm, and clip by gradient norm\n",
    "        params = tf.trainable_variables()\n",
    "        gradients = tf.gradients(self.loss, params)\n",
    "        self.gradient_norm = tf.global_norm(gradients)\n",
    "        clipped_gradients, _ = tf.clip_by_global_norm(gradients, max_gradient_norm)\n",
    "        self.param_norm = tf.global_norm(params)\n",
    "\n",
    "        # Define optimizer and updates\n",
    "        # (updates is what you need to fetch in session.run to do a gradient update)\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=learning_rate) # you can try other optimizers\n",
    "        self.updates = opt.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)\n",
    "        \n",
    "        self.summaries = tf.summary.merge_all()\n",
    "        \n",
    "    def add_embedding_layer(self, emb_matrix):\n",
    "        \"\"\"\n",
    "        Adds word embedding layer to the graph.\n",
    "\n",
    "        Inputs:\n",
    "          emb_matrix: shape (400002, embedding_size).\n",
    "            The GloVe vectors, plus vectors for PAD and UNK.\n",
    "        \"\"\"\n",
    "        embedding_matrix = tf.constant(emb_matrix, dtype=tf.float32, name=\"emb_matrix\")\n",
    "        self.context_embeds = tf.nn.embedding_lookup(embedding_matrix, self.context_ids)\n",
    "        self.question_embeds = tf.nn.embedding_lookup(embedding_matrix, self.question_ids)\n",
    "        \n",
    "    def build_graph(self):\n",
    "        \"\"\"Builds the main part of the graph for the model, starting from the input embeddings \n",
    "           to the final distributions for the answer span.\n",
    "\n",
    "        Defines:\n",
    "          self.logits_start, self.logits_end: Both tensors shape (batch_size, context_len).\n",
    "            These are the logits (i.e. values that are fed into the softmax function) for the start and \n",
    "            end distribution.\n",
    "            Important: these are -large in the pad locations. Necessary for when we feed into the cross \n",
    "            entropy function.\n",
    "          self.probdist_start, self.probdist_end: Both shape (batch_size, context_len). Each row sums to 1.\n",
    "            These are the result of taking (masked) softmax of logits_start and logits_end.\n",
    "        \"\"\"\n",
    "\n",
    "        # Use a RNN to get hidden states for the context and the question\n",
    "        # Note: here the RNNEncoder is shared (i.e. the weights are the same)\n",
    "        # between the context and the question.\n",
    "    \n",
    "        encoder = RNNEncoder(hidden_size=hidden_size_encoder, keep_prob=self.keep_prob)\n",
    "        context_hiddens = encoder.build_graph(self.context_embeds, self.context_mask, scopename='RNNEncoder')\n",
    "        question_hiddens = encoder.build_graph(self.question_embeds, self.question_mask, scopename='RNNEncoder')\n",
    "        \n",
    "        # Use context hidden states to attend to question hidden states - Basic Attention\n",
    "        last_dim = context_hiddens.get_shape().as_list()[-1]\n",
    "        print(\"last dim\", last_dim)\n",
    "\n",
    "        attn_layer = BasicAttn(self.keep_prob, last_dim,\n",
    "                                   last_dim)\n",
    "        _, attn_output = attn_layer.build_graph(question_hiddens, \n",
    "                                                self.question_mask,\n",
    "                                                context_hiddens)  # attn_output is shape (batch_size, context_len, hidden_size*2)\n",
    "\n",
    "        # Concat attn_output to context_hiddens to get blended_reps\n",
    "        blended_reps = tf.concat([context_hiddens, attn_output], axis=2)  # (batch_size, context_len, hidden_size*4)\n",
    "        \n",
    "        # Apply fully connected layer to each blended representation\n",
    "        # Note, tf.contrib.layers.fully_connected applies a ReLU non-linarity here by default\n",
    "        blended_reps_final = tf.contrib.layers.fully_connected(blended_reps, num_outputs=self.hidden_size_fully_connected) # blended_reps_final is shape (batch_size, context_len, hidden_size)\n",
    "\n",
    "        # Use softmax layer to compute probability distribution for start location\n",
    "        # Note this produces self.logits_start and self.probdist_start, both of which have shape (batch_size, context_len)\n",
    "        with vs.variable_scope(\"StartDist\", reuse=tf.AUTO_REUSE):\n",
    "            softmax_layer_start = SimpleSoftmaxLayer()\n",
    "            self.logits_start, self.probdist_start = softmax_layer_start.build_graph(blended_reps_final, self.context_mask)\n",
    "\n",
    "        # Use softmax layer to compute probability distribution for end location\n",
    "        # Note this produces self.logits_end and self.probdist_end, both of which have shape (batch_size, context_len)\n",
    "        with vs.variable_scope(\"EndDist\", reuse=tf.AUTO_REUSE):\n",
    "            softmax_layer_end = SimpleSoftmaxLayer()\n",
    "            self.logits_end, self.probdist_end = softmax_layer_end.build_graph(blended_reps_final, self.context_mask)\n",
    "\n",
    "        \n",
    "    def add_loss(self):\n",
    "        \"\"\"\n",
    "        Add loss computation to the graph.\n",
    "\n",
    "        Uses:\n",
    "              self.logits_start: shape (batch_size, context_len)\n",
    "                IMPORTANT: Assumes that self.logits_start is masked (i.e. has -large in masked locations).\n",
    "                That's because the tf.nn.sparse_softmax_cross_entropy_with_logits\n",
    "                function applies softmax and then computes cross-entropy loss.\n",
    "                So you need to apply masking to the logits (by subtracting large\n",
    "                number in the padding location) BEFORE you pass to the\n",
    "                sparse_softmax_cross_entropy_with_logits function.\n",
    "\n",
    "              self.ans_span: shape (batch_size, 2)\n",
    "                Contains the gold start and end locations\n",
    "\n",
    "            Defines:\n",
    "              self.loss_start, self.loss_end, self.loss: all scalar tensors\n",
    "        \"\"\"\n",
    "        with vs.variable_scope(\"loss\", reuse=tf.AUTO_REUSE):\n",
    "\n",
    "            # Calculate loss for prediction of start position\n",
    "            loss_start = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits_start, labels=self.answer_span[:, 0]) # loss_start has shape (batch_size)\n",
    "            self.loss_start = tf.reduce_mean(loss_start) # scalar. avg across batch\n",
    "            tf.summary.scalar('loss_start', self.loss_start) # log to tensorboard\n",
    "\n",
    "            # Calculate loss for prediction of end position\n",
    "            loss_end = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits_end, labels=self.answer_span[:, 1])\n",
    "            self.loss_end = tf.reduce_mean(loss_end)\n",
    "            tf.summary.scalar('loss_end', self.loss_end)\n",
    "\n",
    "            # Add the two losses\n",
    "            self.loss = self.loss_start + self.loss_end\n",
    "            tf.summary.scalar('loss', self.loss)\n",
    "            \n",
    "    def run_train_iter(self, session, batch):\n",
    "        \"\"\"\n",
    "        This performs a single training iteration (forward pass, loss computation, backprop, parameter update)\n",
    "\n",
    "        Inputs:\n",
    "          session: TensorFlow session\n",
    "          batch: a Batch object\n",
    "          summary_writer: for Tensorboard\n",
    "\n",
    "        Returns:\n",
    "          loss: The loss (averaged across the batch) for this batch.\n",
    "          global_step: The current number of training iterations we've done\n",
    "          param_norm: Global norm of the parameters\n",
    "          gradient_norm: Global norm of the gradients\n",
    "        \"\"\"\n",
    "        # Match up our input data with the placeholders\n",
    "        input_feed = {}\n",
    "        input_feed[self.context_ids] = batch.context_ids\n",
    "        input_feed[self.context_mask] = batch.context_mask\n",
    "        input_feed[self.question_ids] = batch.qn_ids\n",
    "        input_feed[self.question_mask] = batch.qn_mask\n",
    "        input_feed[self.answer_span] = batch.ans_span\n",
    "        input_feed[self.keep_prob] = 1.0 - dropout # apply dropout\n",
    "\n",
    "        # output_feed contains the things we want to fetch.\n",
    "        output_feed = [self.updates, self.summaries, self.loss, self.global_step, self.param_norm, self.gradient_norm]\n",
    "\n",
    "        # Run the model\n",
    "        [_, summaries, loss, global_step, param_norm, gradient_norm] = session.run(fetches=output_feed, feed_dict=input_feed)\n",
    "\n",
    "        return loss, global_step, param_norm, gradient_norm\n",
    "    \n",
    "    def train(self, session, train_context_path, train_qn_path, train_ans_path, dev_qn_path, dev_context_path, dev_ans_path):\n",
    "        \"\"\"\n",
    "        Main training loop.\n",
    "\n",
    "        Inputs:\n",
    "          session: TensorFlow session\n",
    "          {train/dev}_{qn/context/ans}_path: paths to {train/dev}.{context/question/answer} data files\n",
    "        \"\"\"\n",
    "\n",
    "        # Print number of model parameters\n",
    "        tic = time.time()\n",
    "        params = tf.trainable_variables()\n",
    "        num_params = sum(map(lambda t: np.prod(tf.shape(t.value()).eval()), params))\n",
    "        toc = time.time()\n",
    "        logging.info(\"Number of params: %d (retrieval took %f secs)\" % (num_params, toc - tic))\n",
    "\n",
    "        epoch = 0\n",
    "        print_every = 1\n",
    "\n",
    "        logging.info(\"Beginning training loop...\")\n",
    "        while epoch < num_epochs:\n",
    "            epoch += 1\n",
    "            epoch_tic = time.time()\n",
    "\n",
    "            # Loop over batches\n",
    "            for batch in get_batch_generator(self.word2id, train_context_path, train_qn_path, train_ans_path, batch_size, context_len=context_len, question_len=question_len, discard_long=True):\n",
    "\n",
    "                # Run training iteration\n",
    "                iter_tic = time.time()\n",
    "                loss, global_step, param_norm, grad_norm = self.run_train_iter(session, batch)\n",
    "                iter_toc = time.time()\n",
    "                iter_time = iter_toc - iter_tic\n",
    "                \n",
    "                # Sometimes print info to screen\n",
    "                if global_step % print_every == 0:\n",
    "                    logging.info(\n",
    "                        'epoch %d, iter %d, loss %.5f, grad norm %.5f, param norm %.5f, batch time %.3f' %\n",
    "                        (epoch, global_step, loss, grad_norm, param_norm, iter_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the SQuAD_Model...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f94a862f0a6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mqm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQuAD_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memb_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-f4108fd55d39>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, id2word, word2id, emb_matrix)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# build the computational graph (similar to compiling the model?)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-f4108fd55d39>\u001b[0m in \u001b[0;36mbuild_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# between the context and the question.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNNEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_size_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mcontext_hiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscopename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'RNNEncoder'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mquestion_hiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscopename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'RNNEncoder'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-b574f9355623>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hidden_size, keep_prob)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_cell_fw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRUCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m#self.rnn_cell_fw = rnn_cell.GRUCell(num_units=self.hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#self.rnn_cell_fw = DropoutWrapper(self.rnn_cell_fw, input_keep_prob=self.keep_prob)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, activation, recurrent_activation, use_bias, kernel_initializer, recurrent_initializer, bias_initializer, kernel_regularizer, recurrent_regularizer, bias_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, dropout, recurrent_dropout, implementation, reset_after, **kwargs)\u001b[0m\n\u001b[1;32m   1461\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_constraint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_constraint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1464\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent_dropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimplementation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimplementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    651\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m     \"\"\"\n\u001b[0;32m--> 653\u001b[0;31m     raise TypeError(\"Using a `tf.Tensor` as a Python `bool` is not allowed. \"\n\u001b[0m\u001b[1;32m    654\u001b[0m                     \u001b[0;34m\"Use `if t is not None:` instead of `if t:` to test if a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0;34m\"tensor is defined, and use TensorFlow ops such as \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor."
     ]
    }
   ],
   "source": [
    "qm = SQuAD_Model(id2word=id2word, word2id=word2id, emb_matrix=emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get filepaths to train/dev datafiles for tokenized queries, contexts and answers\n",
    "train_context_path = os.path.join(data_dir, \"train.context\")\n",
    "train_qn_path = os.path.join(data_dir, \"train.question\")\n",
    "train_ans_path = os.path.join(data_dir, \"train.span\")\n",
    "dev_context_path = os.path.join(data_dir, \"dev.context\")\n",
    "dev_qn_path = os.path.join(data_dir, \"dev.question\")\n",
    "dev_ans_path = os.path.join(data_dir, \"dev.span\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm.context_embeds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import re\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    qm.train(session, train_context_path, train_qn_path, train_ans_path, dev_qn_path, dev_context_path, dev_ans_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
