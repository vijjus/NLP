{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, concatenate\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(data['event'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = sorted(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dict = {}\n",
    "for i, e in enumerate(events):\n",
    "    if e not in event_dict:\n",
    "        event_dict[e] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Questions:\n",
    "\n",
    "*  Are there any words in this vocabulary that are not in embedding vocabularies?\n",
    "*  Should I use the word piece tokenizer?\n",
    "*  I should try to get a sense of how the words are distributed across labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are many words that only occur in one category, and similarly many that occur in a small number of categories.\n",
    "\n",
    "The first option would be to try an LSTM encoder that feeds into a feedforward layer with a final 48-way softmax to produce the right value.\n",
    "\n",
    "* Input: fixed length vector of text {batch_size x seq_length x embedding_size}\n",
    "* N x GRU unit: taking in each input and producing a final hidden vector without dropout\n",
    "* Feedforward Layer, taking input & output of GRU units to produce output dimension 48\n",
    "* Softmax Layer: final layer for output calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path=\"/Users/vijay/MIDS/w266/Project/AnsweringMachines/dataset/glove.6B.100d.txt\"\n",
    "embedding_size=100\n",
    "text_len=60\n",
    "hidden_size_encoder=200\n",
    "dropout=0.3\n",
    "learning_rate=0.01\n",
    "max_gradient_norm=10\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocab import get_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3790/400000 [00:00<00:10, 37893.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GLoVE vectors from file: /Users/vijay/MIDS/w266/Project/AnsweringMachines/dataset/glove.6B.100d.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:09<00:00, 41948.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load embedding matrix and vocab mappings\n",
    "emb_matrix, word2id, id2word = get_glove(glove_path, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_XY(data):\n",
    "    total_len = len(data)\n",
    "    y = np.zeros((total_len,48))\n",
    "    X = np.zeros((total_len,60,100))\n",
    "    for i in range(total_len):\n",
    "        y[i][event_dict[data.iloc[i]['event']]] = 1\n",
    "        text = data.iloc[i]['text'].lower()\n",
    "        tokens = text.split(\" \")\n",
    "        for j, token in enumerate(tokens):\n",
    "            if token in word2id:\n",
    "                X[i][j] = emb_matrix[word2id[token]]\n",
    "            else:\n",
    "                X[i][j] = np.random.normal(size=100)\n",
    "    return X, y               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = get_XY(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153956, 60, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153956, 48)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X[:100000], Y[:100000]\n",
    "X_test, y_test = Y[100000:], Y[100000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 60, 100)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(60,100))\n",
    "output_1 = LSTM(100)(inputs)\n",
    "output_2 = LSTM(100,go_backwards=True)(inputs)\n",
    "d = concatenate([output_1, output_2])\n",
    "d = Dense(100)(d)\n",
    "predictions = Dense(48, activation='sigmoid', name='main_output')(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=32, steps_per_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
